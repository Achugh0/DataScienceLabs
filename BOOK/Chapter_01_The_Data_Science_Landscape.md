# Chapter 1
# The Data Science Landscape

> *"Data is the new oil. It's valuable, but if unrefined it cannot really be used."*  
> â€” Clive Humby, Data Science Innovator

---

## Chapter Overview

Welcome to your journey into data science! This chapter introduces you to the exciting field of data science, exploring what it is, why it matters, and how you can build a successful career in this rapidly growing domain.

**In this chapter, you will:**
- Understand what data science really is (beyond the buzzwords)
- Learn the four foundational pillars of data science
- Explore diverse career paths and roles
- Master the data science workflow
- See real-world applications across industries

**Time Required:** 4-6 hours  
**Labs:** 4 hands-on exercises

---

## 1.1 What Is Data Science?

Data science is the interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines elements from mathematics, statistics, computer science, and domain expertise to solve complex problems.

### The Evolution of Data Science

Data science didn't emerge overnight. Let's trace its evolution:

**1960s-1970s: The Database Era**
- Organizations began storing data electronically
- Focus on data storage and retrieval
- SQL and relational databases emerged

**1980s-1990s: Business Intelligence**
- Data warehousing became prevalent
- Reporting and dashboards emerged
- Descriptive analytics dominated

**2000s: The Big Data Revolution**
- Explosion of digital data (social media, sensors, mobile)
- Hadoop and distributed computing
- Need for new analytical approaches

**2010s-Present: The AI Era**
- Machine learning becomes mainstream
- Deep learning breakthroughs
- Data science as a distinct profession
- Democratization of ML tools

### Defining Data Science Today

Modern data science encompasses three key activities:

**1. Exploration** ğŸ”
```
Raw Data â†’ Clean Data â†’ Insights â†’ Hypotheses
```

**2. Prediction** ğŸ¯
```
Historical Data â†’ Model Training â†’ Future Predictions â†’ Decisions
```

**3. Action** âš¡
```
Insights â†’ Recommendations â†’ Implementation â†’ Impact
```

<div class="callout callout-info">
<h4>ğŸ’¡ Key Insight</h4>
<p>Data science is not just about building modelsâ€”it's about solving business problems. The best data scientists spend 80% of their time understanding problems and preparing data, and only 20% building models.</p>
</div>

### Data Science vs. Related Fields

It's important to understand how data science relates to adjacent fields:

| Field | Primary Focus | Key Skills | Example Task |
|-------|--------------|------------|--------------|
| **Data Science** | Extracting insights and building predictive models | Python, ML, Statistics, Domain knowledge | Predict customer churn |
| **Data Analytics** | Analyzing historical data to answer questions | SQL, Excel, BI tools, Statistics | Analyze sales trends |
| **Machine Learning** | Building and optimizing algorithms | Python, Math, Algorithm design | Build recommendation engine |
| **Data Engineering** | Building data infrastructure | SQL, ETL, Cloud platforms | Build data pipelines |
| **Business Intelligence** | Reporting and dashboards | SQL, Tableau, Power BI | Create executive dashboard |

<div class="callout callout-warning">
<h4>âš ï¸ Common Misconception</h4>
<p>Many people think data science is only about machine learning. In reality, successful data scientists need a broad skill set including communication, business acumen, and software engineering.</p>
</div>

---

## 1.2 The Four Pillars of Data Science

Every successful data scientist masters four foundational areas:

### Pillar 1: Mathematics & Statistics ğŸ“Š

**Why It Matters:**
Statistics provides the theoretical foundation for understanding uncertainty, making inferences, and validating conclusions.

**Essential Concepts:**
- **Descriptive Statistics**: Mean, median, mode, variance, standard deviation
- **Probability**: Distributions, conditional probability, Bayes' theorem
- **Inferential Statistics**: Hypothesis testing, confidence intervals, p-values
- **Linear Algebra**: Vectors, matrices, eigenvalues (for ML)
- **Calculus**: Derivatives, gradients (for optimization)

**Real-World Example:**
```python
# A/B test to determine if new website design increases conversions
from scipy import stats

control_conversions = [0, 1, 0, 0, 1, 1, 0, 1, 0, 1]  # 50% conversion
treatment_conversions = [1, 1, 0, 1, 1, 1, 0, 1, 1, 1]  # 70% conversion

# Statistical test
statistic, pvalue = stats.ttest_ind(control_conversions, treatment_conversions)

if pvalue < 0.05:
    print("âœ… New design significantly improves conversions!")
else:
    print("âŒ No significant difference detected")
```

### Pillar 2: Programming & Software Engineering ğŸ’»

**Why It Matters:**
You need to translate ideas into working code that processes data, builds models, and delivers value at scale.

**Essential Skills:**
- **Python**: The lingua franca of data science
- **SQL**: For working with databases
- **Git**: Version control for collaboration
- **Software Engineering**: Writing clean, maintainable, testable code
- **Cloud Platforms**: AWS, GCP, or Azure

**Code Quality Matters:**
```python
# âŒ Bad: Hard to understand and maintain
def p(d):
    return sum(d)/len(d)

# âœ… Good: Clear, documented, and professional
def calculate_mean(values: list[float]) -> float:
    """
    Calculate the arithmetic mean of a list of numbers.
    
    Args:
        values: List of numerical values
        
    Returns:
        The mean of the input values
        
    Raises:
        ValueError: If the list is empty
    """
    if not values:
        raise ValueError("Cannot calculate mean of empty list")
    return sum(values) / len(values)
```

### Pillar 3: Domain Expertise ğŸ“

**Why It Matters:**
Understanding the business context ensures you solve the *right* problem, not just *a* problem.

**Key Areas:**
- **Industry Knowledge**: Healthcare, finance, retail, etc.
- **Business Metrics**: Revenue, churn, lifetime value, etc.
- **Stakeholder Management**: Communicating with non-technical audiences
- **Problem Formulation**: Translating business problems to data science solutions

**Case Study: Healthcare Readmission Prediction**

A hospital wants to reduce 30-day readmissions:

```
âŒ Wrong Approach:
"Let's build the most accurate ML model!"

âœ… Right Approach:
1. Understand the problem:
   - Why do patients return?
   - What interventions are available?
   - What are the costs of false positives/negatives?

2. Define success metrics:
   - Not just model accuracy
   - Actual reduction in readmissions
   - Return on investment
   - Clinician adoption rate

3. Build interpretable model:
   - Doctors need to understand predictions
   - Actionable insights more valuable than perfect accuracy
```

### Pillar 4: Communication & Storytelling ğŸ“¢

**Why It Matters:**
Even the best analysis is worthless if you can't convince others to act on it.

**Essential Skills:**
- **Data Visualization**: Creating clear, compelling charts
- **Presentation**: Delivering insights to executives
- **Writing**: Documenting findings and recommendations
- **Storytelling**: Crafting narratives around data

**Example: Converting Analysis to Action**

```
âŒ Technical-only communication:
"The random forest model achieved 87% accuracy with an AUC of 0.92. 
Feature importance analysis shows that recency, frequency, and monetary 
values are the top predictors."

âœ… Business-focused communication:
"We can identify 85% of customers who will churn next month. By targeting 
these customers with retention offers, we project saving $2.3M annually.

Key findings:
â€¢ Customers who haven't purchased in 60+ days are 5x more likely to churn
â€¢ High-value customers (>$1000/year) need special attention
â€¢ Proactive outreach reduces churn by 40%

Recommendation: Launch targeted retention campaign for top 1,000 at-risk customers."
```

<div class="callout callout-success">
<h4>âœ… Best Practice</h4>
<p>The "So What?" Test: For every insight you present, ask "So what?" If you can't articulate the business impact in one sentence, you're not ready to present.</p>
</div>

---

## 1.3 Data Science Roles and Career Paths

The data science field offers diverse career opportunities. Let's explore the major roles:

### The Data Science Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Chief Data Officer (CDO)           â”‚
â”‚      (Strategic Leadership)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–²
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚    Data Science Manager / Director      â”‚
â”‚    (Team Leadership, Strategy)          â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–²
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚         Senior Data Scientist               â”‚
â”‚    (Complex Projects, Mentorship)           â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–²
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚          Data Scientist                     â”‚
â”‚    (ML Models, Analysis, Insights)          â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–²
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚       Junior Data Scientist                 â”‚
â”‚    (Analysis, Basic ML, Learning)           â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Role Descriptions

#### 1. Data Analyst ğŸ“ˆ

**Focus:** Analyzing historical data to answer specific questions

**Responsibilities:**
- Create reports and dashboards
- Perform SQL queries
- Conduct statistical analysis
- Present findings to stakeholders

**Skills Required:**
- SQL (advanced)
- Excel / Google Sheets
- BI tools (Tableau, Power BI)
- Basic statistics
- Data visualization

**Typical Salary:** $60K - $90K

**Day in the Life:**
```
9:00 AM  - Review overnight data quality issues
10:00 AM - Build dashboard for marketing team
12:00 PM - Lunch with product manager
1:00 PM  - Analyze A/B test results
3:00 PM  - Present findings to stakeholders
4:00 PM  - Document analysis methodology
```

#### 2. Data Scientist ğŸ”¬

**Focus:** Building predictive models and extracting insights

**Responsibilities:**
- Design and conduct experiments
- Build machine learning models
- Perform statistical analysis
- Collaborate with engineering on deployment
- Communicate insights to business stakeholders

**Skills Required:**
- Python / R
- Machine learning (scikit-learn)
- Statistics (hypothesis testing, A/B testing)
- SQL
- Data visualization

**Typical Salary:** $90K - $140K

**Day in the Life:**
```
9:00 AM  - Review model performance metrics
10:00 AM - Feature engineering for churn model
12:00 PM - Meeting with product team
1:00 PM  - Train and evaluate multiple models
3:00 PM  - Document model performance
4:00 PM  - Prepare presentation for stakeholders
```

#### 3. Machine Learning Engineer ğŸ¤–

**Focus:** Deploying and scaling ML models to production

**Responsibilities:**
- Build scalable ML pipelines
- Deploy models to production
- Optimize model performance
- Monitor model health
- Implement MLOps practices

**Skills Required:**
- Python (advanced)
- ML frameworks (TensorFlow, PyTorch)
- Software engineering
- Docker, Kubernetes
- Cloud platforms (AWS, GCP)
- MLOps tools

**Typical Salary:** $120K - $180K

**Day in the Life:**
```
9:00 AM  - Review model performance alerts
10:00 AM - Optimize inference latency
12:00 PM - Code review for new ML pipeline
1:00 PM  - Deploy new model version
3:00 PM  - Set up monitoring dashboards
4:00 PM  - Plan next sprint with team
```

#### 4. Data Engineer ğŸ—ï¸

**Focus:** Building and maintaining data infrastructure

**Responsibilities:**
- Design data pipelines
- Maintain data warehouses
- Ensure data quality
- Optimize query performance
- Build ETL processes

**Skills Required:**
- SQL (expert level)
- Python / Scala
- Spark, Kafka
- Cloud data services
- Database design

**Typical Salary:** $100K - $150K

#### 5. Research Scientist (AI/ML) ğŸ§ 

**Focus:** Advancing state-of-the-art ML techniques

**Responsibilities:**
- Conduct research on novel ML approaches
- Publish papers
- Prototype new algorithms
- Collaborate with product teams

**Skills Required:**
- PhD or equivalent
- Deep learning expertise
- Mathematics (advanced)
- Research methodology
- Python, PyTorch/TensorFlow

**Typical Salary:** $150K - $250K+

<div class="callout callout-tip">
<h4>ğŸ’¼ Career Advice</h4>
<p><strong>Starting Out:</strong> Most people begin as Data Analysts or Junior Data Scientists. Focus on building strong fundamentals in SQL, Python, and statistics.</p>

<p><strong>2-3 Years:</strong> Transition to Data Scientist role. Deepen ML skills and start building end-to-end projects.</p>

<p><strong>5+ Years:</strong> Specialize (ML Engineering, Research) or move into leadership (Manager, Director).</p>
</div>

---

## 1.4 The Data Science Workflow

Every data science project follows a similar workflow. Understanding this process is crucial for success.

### The Data Science Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SCIENCE LIFECYCLE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. PROBLEM DEFINITION
   "What question are we trying to answer?"
   â†“
2. DATA COLLECTION
   "What data do we need?"
   â†“
3. DATA EXPLORATION
   "What patterns exist in the data?"
   â†“
4. DATA PREPARATION
   "How do we clean and transform the data?"
   â†“
5. MODELING
   "What algorithms should we use?"
   â†“
6. EVALUATION
   "How well does our solution work?"
   â†“
7. DEPLOYMENT
   "How do we put this into production?"
   â†“
8. MONITORING
   "Is it still working well?"
   â†“
9. ITERATION â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   "What can we improve?"
```

Let's explore each phase in detail:

### Phase 1: Problem Definition (Most Important!)

**Time Allocation:** 10-20% of project  
**Goal:** Clearly define the business problem and success criteria

**Key Questions:**
- What business problem are we solving?
- Who are the stakeholders?
- What does success look like?
- What are the constraints (time, budget, data)?
- What's the baseline (current state)?

**Example:**
```
âŒ Vague: "Improve customer retention"

âœ… Specific: 
"Reduce customer churn from 5% to 3% monthly by identifying 
at-risk customers 30 days before they leave, enabling proactive 
intervention with targeted retention offers."

Success Metrics:
- Churn rate reduction: 5% â†’ 3%
- Model recall: >80% (catch most churners)
- Precision: >70% (minimize false alarms)
- ROI: >3x (retention value vs intervention cost)
```

### Phase 2: Data Collection

**Time Allocation:** 10-15% of project  
**Goal:** Gather all necessary data

**Data Sources:**
- Internal databases (SQL)
- APIs (web scraping, third-party)
- Files (CSV, JSON, Excel)
- Streaming data (real-time events)
- External datasets (public data, purchased data)

**Example Python Code:**
```python
import pandas as pd
import requests

# Load from database
import sqlite3
conn = sqlite3.connect('customer_db.db')
customers = pd.read_sql("SELECT * FROM customers", conn)

# Load from API
response = requests.get('https://api.example.com/transactions')
transactions = pd.DataFrame(response.json())

# Load from file
support_tickets = pd.read_csv('support_tickets.csv')

# Combine datasets
full_data = customers.merge(transactions, on='customer_id')
full_data = full_data.merge(support_tickets, on='customer_id')

print(f"Collected {len(full_data):,} records")
```

### Phase 3: Data Exploration (EDA)

**Time Allocation:** 20-30% of project  
**Goal:** Understand data patterns, distributions, and relationships

**Key Activities:**
- Summary statistics
- Distribution analysis
- Correlation analysis
- Outlier detection
- Missing data assessment

**Example Analysis:**
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Summary statistics
print(customers.describe())

# Distribution of key variable
plt.figure(figsize=(10, 6))
plt.hist(customers['total_purchases'], bins=50, edgecolor='black')
plt.title('Distribution of Customer Purchases')
plt.xlabel('Total Purchases ($)')
plt.ylabel('Frequency')
plt.show()

# Correlation matrix
correlation = customers[['age', 'income', 'purchases', 'tenure']].corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Feature Correlations')
plt.show()

# Key insights
print(f"Average customer lifetime: {customers['tenure'].mean():.1f} months")
print(f"Churn rate: {customers['churned'].mean():.1%}")
print(f"Missing data: {customers.isnull().sum().sum()} values")
```

<div class="callout callout-info">
<h4>ğŸ“Š EDA Best Practices</h4>
<ol>
<li><strong>Always visualize:</strong> Charts reveal patterns numbers hide</li>
<li><strong>Check assumptions:</strong> Don't assume data is clean</li>
<li><strong>Document findings:</strong> Keep a notebook of insights</li>
<li><strong>Collaborate:</strong> Share interesting patterns with domain experts</li>
</ol>
</div>

### Phase 4: Data Preparation (Most Time-Consuming!)

**Time Allocation:** 50-60% of project  
**Goal:** Transform raw data into ML-ready features

**Common Tasks:**
1. Handle missing values
2. Remove duplicates
3. Fix data types
4. Encode categorical variables
5. Scale numerical features
6. Create new features
7. Split train/test sets

**Example Code:**
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Handle missing values
imputer = SimpleImputer(strategy='median')
customers['income'] = imputer.fit_transform(customers[['income']])

# Encode categorical variables
customers = pd.get_dummies(customers, columns=['region', 'product_type'])

# Create new features
customers['avg_purchase_value'] = customers['total_revenue'] / customers['num_purchases']
customers['days_since_last_purchase'] = (pd.Timestamp.now() - customers['last_purchase_date']).dt.days

# Remove outliers
Q1 = customers['total_revenue'].quantile(0.25)
Q3 = customers['total_revenue'].quantile(0.75)
IQR = Q3 - Q1
customers = customers[
    (customers['total_revenue'] >= Q1 - 1.5 * IQR) &
    (customers['total_revenue'] <= Q3 + 1.5 * IQR)
]

# Split data
X = customers.drop('churned', axis=1)
y = customers['churned']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set: {len(X_train):,} samples")
print(f"Test set: {len(X_test):,} samples")
```

### Phase 5: Modeling

**Time Allocation:** 10-20% of project  
**Goal:** Build and train predictive models

**Approach:**
1. Start simple (baseline model)
2. Try multiple algorithms
3. Tune hyperparameters
4. Ensemble if needed

**Example:**
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Baseline: Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train_scaled, y_train)
lr_pred = lr_model.predict(X_test_scaled)

print("Logistic Regression Results:")
print(f"Accuracy: {accuracy_score(y_test, lr_pred):.3f}")
print(f"Precision: {precision_score(y_test, lr_pred):.3f}")
print(f"Recall: {recall_score(y_test, lr_pred):.3f}")

# Advanced: Random Forest
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10)
rf_model.fit(X_train_scaled, y_train)
rf_pred = rf_model.predict(X_test_scaled)

print("\nRandom Forest Results:")
print(f"Accuracy: {accuracy_score(y_test, rf_pred):.3f}")
print(f"Precision: {precision_score(y_test, rf_pred):.3f}")
print(f"Recall: {recall_score(y_test, rf_pred):.3f}")
```

---

## Chapter Summary

In this chapter, you learned:

âœ… **Data science definition**: An interdisciplinary field combining statistics, programming, and domain expertise to extract insights from data

âœ… **Four pillars**: Mathematics/Statistics, Programming, Domain Expertise, and Communication

âœ… **Career paths**: From Data Analyst to Research Scientist, with clear progression

âœ… **The workflow**: 9-phase lifecycle from problem definition to deployment

âœ… **Key insight**: 80% of data science is understanding problems and preparing data; only 20% is modeling

### Key Takeaways

1. **Start with the problem, not the data**: Great data scientists solve business problems, not just build models

2. **Communication is crucial**: Technical skills matter, but so does explaining your work to non-technical audiences

3. **Iteration is normal**: Data science is messy and non-linear. Expect to revisit earlier phases

4. **Tools are secondary**: Focus on fundamentals. Specific tools change, but principles remain constant

---

## Coming Up Next

In **Chapter 2**, we'll dive into the mindset of successful data scientists, exploring the five habits that separate great practitioners from average ones.

But first, let's put what you've learned into practice with four hands-on labs!

---

## Hands-On Labs

The following labs will help you apply the concepts from this chapter:

- **Lab 1**: Exploring Data Science Roles (page 38)
- **Lab 2**: The Data Science Workflow (page 45)
- **Lab 3**: Data Types and Sources (page 52)
- **Lab 4**: Environment Setup (page 60)

Complete all four labs before moving to Chapter 2.

---

## Further Reading

ğŸ“š **Books:**
- *The Data Science Handbook* by Field Cady
- *Data Science for Business* by Foster Provost
- *Storytelling with Data* by Cole Nussbaumer Knaflic

ğŸ”— **Online Resources:**
- Kaggle Learn: www.kaggle.com/learn
- Towards Data Science: towardsdatascience.com
- Data Science Central: www.datasciencecentral.com

ğŸ¥ **Videos:**
- "What is Data Science?" by IBM
- "A Day in the Life of a Data Scientist"
- "Data Science Career Paths Explained"

---

**Ready to start your first lab? Turn the page!** â†’
