# Lab 73-102+: Capstone Projects & Career Launch

## Chapter 16-17: Portfolio & Career Development

### 30+ Final Labs for Career Success

---

# Lab 73-82: Capstone Projects (10 Complete Projects)

## Lab 73: E-commerce Recommendation System
```python
"""
Complete end-to-end recommendation engine

Components:
- Data pipeline (collect, clean, transform)
- Collaborative filtering algorithm
- Content-based filtering
- Hybrid approach
- A/B testing framework
- Production deployment
- Monitoring dashboard

Deliverables:
- Jupyter notebook with analysis
- Python package with API
- Docker container
- Documentation
- Performance report
"""

# Example: Collaborative Filtering
import pandas as pd
import numpy as np
from scipy.sparse.linalg import svds

class CollaborativeFilter:
    def __init__(self, n_factors=50):
        self.n_factors = n_factors
        self.user_factors = None
        self.item_factors = None
    
    def fit(self, user_item_matrix):
        # SVD decomposition
        U, sigma, Vt = svds(user_item_matrix, k=self.n_factors)
        
        self.user_factors = U
        self.item_factors = Vt.T
        self.sigma = np.diag(sigma)
    
    def predict(self, user_id, item_id):
        user_vec = self.user_factors[user_id]
        item_vec = self.item_factors[item_id]
        return np.dot(np.dot(user_vec, self.sigma), item_vec)
    
    def recommend(self, user_id, top_n=10):
        user_vec = self.user_factors[user_id]
        scores = np.dot(np.dot(user_vec, self.sigma), self.item_factors.T)
        top_items = np.argsort(scores)[::-1][:top_n]
        return top_items, scores[top_items]

# Build complete system with evaluation metrics
```

## Lab 74: Financial Risk Prediction
```python
"""
Credit risk assessment system

Features:
- Historical loan data analysis
- Feature engineering (100+ features)
- Multiple model comparison
- Ensemble methods
- Explainability (SHAP values)
- Fairness analysis
- Regulatory compliance

Dataset: Lending Club or synthetic data
Target: Predict default probability
"""

import shap

class CreditRiskModel:
    def __init__(self):
        self.models = {}
        self.explainer = None
    
    def engineer_features(self, df):
        """Create 100+ features"""
        features = df.copy()
        
        # Ratio features
        features['debt_to_income'] = df['debt'] / df['income']
        features['payment_to_income'] = df['monthly_payment'] / df['monthly_income']
        
        # Historical features
        features['avg_payment_last_6m'] = df.groupby('customer_id')['payment'].transform(
            lambda x: x.rolling(6, min_periods=1).mean()
        )
        
        # Delinquency features
        features['days_since_last_delinquency'] = (
            pd.Timestamp.now() - df['last_delinquency_date']
        ).dt.days
        
        return features
    
    def train_ensemble(self, X_train, y_train):
        """Train multiple models"""
        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
        from sklearn.linear_model import LogisticRegression
        
        self.models['rf'] = RandomForestClassifier(n_estimators=200)
        self.models['gb'] = GradientBoostingClassifier(n_estimators=200)
        self.models['lr'] = LogisticRegression()
        
        for name, model in self.models.items():
            model.fit(X_train, y_train)
    
    def explain_prediction(self, X):
        """SHAP explanations"""
        self.explainer = shap.TreeExplainer(self.models['rf'])
        shap_values = self.explainer.shap_values(X)
        return shap_values
```

## Lab 75: Healthcare Prediction System
```python
"""
Patient readmission prediction

Components:
- EHR data processing
- Time series features
- Survival analysis
- Deep learning model (LSTM)
- Privacy compliance (HIPAA)
- Clinical validation
"""
```

## Lab 76: Computer Vision - Object Detection
```python
"""
Real-time object detection system

Technology:
- Transfer learning (YOLO, Faster R-CNN)
- Custom dataset creation
- Data augmentation
- Model optimization
- Edge deployment
- Performance benchmarking
"""

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2

def build_detector():
    base_model = MobileNetV2(
        input_shape=(224, 224, 3),
        include_top=False,
        weights='imagenet'
    )
    
    # Freeze base layers
    base_model.trainable = False
    
    # Add detection head
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    
    return model
```

## Lab 77: NLP - Sentiment Analysis at Scale
```python
"""
Twitter sentiment analysis pipeline

Pipeline:
- Data collection (Twitter API)
- Text preprocessing
- Transformer models (BERT)
- Real-time processing (Spark Streaming)
- Sentiment trends dashboard
- Topic modeling
"""

from transformers import BertTokenizer, BertForSequenceClassification
import torch

class SentimentAnalyzer:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased',
            num_labels=3  # negative, neutral, positive
        )
    
    def predict(self, text):
        inputs = self.tokenizer(text, return_tensors='pt', 
                               padding=True, truncation=True)
        outputs = self.model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        return probs.detach().numpy()[0]
    
    def analyze_stream(self, spark_stream):
        """Process streaming tweets"""
        return spark_stream.map(lambda tweet: {
            'text': tweet,
            'sentiment': self.predict(tweet)
        })
```

## Lab 78: Time Series Forecasting - Sales Prediction
```python
"""
Multi-step sales forecasting

Methods:
- ARIMA/SARIMA
- Prophet (Facebook)
- LSTM neural networks
- Ensemble approach
- Uncertainty quantification
- Automated hyperparameter tuning
"""

from prophet import Prophet

class SalesForecaster:
    def __init__(self):
        self.model = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=False
        )
        self.model.add_country_holidays(country_name='US')
    
    def fit(self, df):
        """df must have 'ds' and 'y' columns"""
        self.model.fit(df)
    
    def forecast(self, periods=30):
        future = self.model.make_future_dataframe(periods=periods)
        forecast = self.model.predict(future)
        return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
    
    def plot_components(self):
        fig = self.model.plot_components(self.forecast)
        return fig
```

## Lab 79: Customer Segmentation & LTV
```python
"""
Advanced customer analytics

Analysis:
- RFM segmentation
- K-means clustering
- Customer Lifetime Value prediction
- Churn prediction
- Next-best-action recommendations
- Cohort analysis
"""

class CustomerAnalytics:
    def calculate_rfm(self, df):
        """Calculate RFM scores"""
        now = df['date'].max()
        
        rfm = df.groupby('customer_id').agg({
            'date': lambda x: (now - x.max()).days,  # Recency
            'order_id': 'count',  # Frequency
            'revenue': 'sum'  # Monetary
        })
        
        rfm.columns = ['recency', 'frequency', 'monetary']
        
        # Score each dimension (1-5)
        rfm['r_score'] = pd.qcut(rfm['recency'], 5, labels=[5,4,3,2,1])
        rfm['f_score'] = pd.qcut(rfm['frequency'], 5, labels=[1,2,3,4,5])
        rfm['m_score'] = pd.qcut(rfm['monetary'], 5, labels=[1,2,3,4,5])
        
        rfm['rfm_score'] = rfm['r_score'].astype(str) + \
                           rfm['f_score'].astype(str) + \
                           rfm['m_score'].astype(str)
        
        return rfm
    
    def predict_ltv(self, customer_features):
        """Predict customer lifetime value"""
        from sklearn.ensemble import GradientBoostingRegressor
        
        model = GradientBoostingRegressor()
        model.fit(customer_features, ltv_actual)
        
        ltv_pred = model.predict(customer_features)
        return ltv_pred
```

## Lab 80: A/B Testing Framework
```python
"""
Complete experimentation platform

Features:
- Experiment design
- Sample size calculation
- Statistical testing
- Bayesian A/B testing
- Multi-armed bandit
- Sequential testing
- Automated reporting
"""

import scipy.stats as stats

class ABTest:
    def __init__(self, alpha=0.05, power=0.8):
        self.alpha = alpha
        self.power = power
    
    def calculate_sample_size(self, baseline_rate, mde):
        """Minimum Detectable Effect"""
        from statsmodels.stats.power import zt_ind_solve_power
        
        effect_size = mde / np.sqrt(baseline_rate * (1 - baseline_rate))
        sample_size = zt_ind_solve_power(
            effect_size=effect_size,
            alpha=self.alpha,
            power=self.power,
            ratio=1.0,
            alternative='two-sided'
        )
        
        return int(np.ceil(sample_size))
    
    def analyze_results(self, control, treatment):
        """Statistical analysis"""
        # Frequentist approach
        stat, pvalue = stats.ttest_ind(control, treatment)
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt((control.std()**2 + treatment.std()**2) / 2)
        cohens_d = (treatment.mean() - control.mean()) / pooled_std
        
        # Confidence interval
        ci = stats.t.interval(
            1 - self.alpha,
            len(control) + len(treatment) - 2,
            loc=treatment.mean() - control.mean(),
            scale=stats.sem([control, treatment])
        )
        
        return {
            'pvalue': pvalue,
            'significant': pvalue < self.alpha,
            'effect_size': cohens_d,
            'confidence_interval': ci,
            'lift': (treatment.mean() - control.mean()) / control.mean()
        }
```

## Lab 81: Real-time Dashboard
```python
"""
Live analytics dashboard

Stack:
- Streamlit or Dash
- Real-time data connections
- Interactive visualizations
- Drill-down capabilities
- Export functionality
- Mobile responsive
"""

import streamlit as st
import plotly.express as px

st.set_page_config(page_title="Sales Dashboard", layout="wide")

st.title("ðŸ“Š Real-Time Sales Dashboard")

# Sidebar filters
region = st.sidebar.multiselect("Region", ['North', 'South', 'East', 'West'])
date_range = st.sidebar.date_input("Date Range")

# Metrics
col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Revenue", "$1.2M", "+15%")
col2.metric("Orders", "3,542", "+8%")
col3.metric("Avg Order Value", "$340", "+2%")
col4.metric("Customers", "1,234", "-5%")

# Charts
st.plotly_chart(px.line(df, x='date', y='revenue', color='region'))
st.plotly_chart(px.bar(df, x='product', y='quantity'))

# Data table
st.dataframe(df)
```

## Lab 82: End-to-End ML Platform
```python
"""
Complete MLOps platform

Components:
- Data versioning (DVC)
- Experiment tracking (MLflow)
- Model registry
- Feature store
- Training pipeline
- Deployment automation
- Monitoring & alerting
- Cost optimization
"""
```

---

# Lab 83-92: Interview Preparation

## Lab 83: Coding Challenges
```python
# 20 common data science coding problems

# 1. Moving average
def moving_average(arr, window):
    return [sum(arr[i:i+window])/window 
            for i in range(len(arr)-window+1)]

# 2. Find duplicates
def find_duplicates(arr):
    seen = set()
    dups = set()
    for x in arr:
        if x in seen:
            dups.add(x)
        seen.add(x)
    return list(dups)

# 3. Merge sorted arrays
def merge_sorted(arr1, arr2):
    result = []
    i = j = 0
    while i < len(arr1) and j < len(arr2):
        if arr1[i] < arr2[j]:
            result.append(arr1[i])
            i += 1
        else:
            result.append(arr2[j])
            j += 1
    result.extend(arr1[i:])
    result.extend(arr2[j:])
    return result

# ... 17 more problems
```

## Lab 84-86: SQL Interview Questions
```sql
-- 30 common SQL interview questions

-- 1. Second highest salary
SELECT MAX(salary) FROM employees
WHERE salary < (SELECT MAX(salary) FROM employees);

-- 2. Duplicate emails
SELECT email, COUNT(*) 
FROM users
GROUP BY email
HAVING COUNT(*) > 1;

-- 3. Running total
SELECT date, amount,
       SUM(amount) OVER (ORDER BY date) as running_total
FROM transactions;

-- 4. Month-over-month growth
WITH monthly AS (
    SELECT DATE_TRUNC('month', date) as month,
           SUM(revenue) as revenue
    FROM sales
    GROUP BY month
)
SELECT month, revenue,
       LAG(revenue) OVER (ORDER BY month) as prev_month,
       (revenue - LAG(revenue) OVER (ORDER BY month)) / 
       LAG(revenue) OVER (ORDER BY month) * 100 as growth_pct
FROM monthly;

-- 5. Find gaps in sequences
SELECT t1.id + 1 as gap_start
FROM sequences t1
LEFT JOIN sequences t2 ON t1.id + 1 = t2.id
WHERE t2.id IS NULL AND t1.id < (SELECT MAX(id) FROM sequences);

-- ... 25 more questions
```

## Lab 87-88: Statistics & Probability
```python
"""
50 statistics interview questions

Topics:
- Probability distributions
- Hypothesis testing
- A/B testing
- Bayesian inference
- Sampling methods
- Experimental design
"""

# Example: Bayesian A/B Test
def bayesian_ab_test(control_successes, control_trials,
                     treatment_successes, treatment_trials):
    """
    Bayesian A/B test using Beta distribution
    """
    from scipy.stats import beta
    
    # Posterior distributions
    alpha_c = control_successes + 1
    beta_c = control_trials - control_successes + 1
    
    alpha_t = treatment_successes + 1
    beta_t = treatment_trials - treatment_successes + 1
    
    # Sample from posteriors
    samples_c = beta.rvs(alpha_c, beta_c, size=10000)
    samples_t = beta.rvs(alpha_t, beta_t, size=10000)
    
    # Probability treatment > control
    prob_t_better = (samples_t > samples_c).mean()
    
    # Expected lift
    expected_lift = (samples_t - samples_c).mean()
    
    return {
        'prob_treatment_better': prob_t_better,
        'expected_lift': expected_lift,
        '95%_credible_interval': np.percentile(samples_t - samples_c, [2.5, 97.5])
    }
```

## Lab 89-90: ML System Design
```python
"""
Design challenges:

1. Design YouTube recommendation system
2. Design fraud detection system
3. Design search ranking
4. Design news feed
5. Design ad targeting system
6. Design real-time bidding
7. Design content moderation
8. Design image search
9. Design voice assistant
10. Design autonomous vehicle perception

Template for each:
- Requirements gathering
- Data pipeline design
- Model architecture
- Scaling strategy
- Evaluation metrics
- A/B testing plan
- Monitoring approach
"""
```

## Lab 91-92: Behavioral Questions
```python
"""
STAR method practice:

- Tell me about a time you dealt with ambiguity
- Describe a project where you failed
- How do you prioritize competing deadlines?
- Tell me about a conflict with a teammate
- Describe your most challenging project
- How do you handle stakeholder pushback?
- Tell me about a time you learned something new quickly
- Describe a project with business impact

For each question, prepare:
- Situation
- Task
- Action
- Result (quantified)
"""
```

---

# Lab 93-102+: Portfolio & Career Materials

## Lab 93-95: GitHub Portfolio
```markdown
# Build professional GitHub presence

## Lab 93: README.md optimization
- Clear project descriptions
- Installation instructions
- Usage examples
- Results & metrics
- Visualizations
- Badges (build status, coverage)

## Lab 94: Documentation
- Docstrings
- API documentation (Sphinx)
- Jupyter notebooks
- Blog posts

## Lab 95: Open source contributions
- Find projects
- Submit issues
- Create pull requests
- Code reviews
```

## Lab 96-97: Personal Website
```html
<!-- Portfolio website structure -->

<sections>
  <header>
    - Name & title
    - Contact info
    - Social links (LinkedIn, GitHub, Twitter)
  </header>
  
  <about>
    - 3-sentence bio
    - Skills summary
    - Current focus
  </about>
  
  <projects>
    - 5-10 best projects
    - For each: title, description, tech stack, results, link
    - Screenshots/videos
  </projects>
  
  <blog>
    - Technical articles
    - Project walkthroughs
    - Tutorial posts
  </blog>
  
  <resume>
    - Downloadable PDF
    - Web version
  </resume>
</sections>
```

## Lab 98: Resume Optimization
```
DATA SCIENTIST

SUMMARY
Results-driven data scientist with 3+ years building ML models that generated $5M+ in value. Expert in Python, SQL, and cloud platforms. Proven track record deploying production systems.

TECHNICAL SKILLS
- Languages: Python, SQL, R, Scala
- ML/DL: Scikit-learn, TensorFlow, PyTorch, XGBoost
- Big Data: Spark, Hadoop, Kafka
- Cloud: AWS (SageMaker, EMR), GCP, Azure
- Tools: Docker, Kubernetes, Git, MLflow

EXPERIENCE

Senior Data Scientist | Company | 2021-Present
â€¢ Built recommendation engine increasing revenue by 15% ($3M annually)
â€¢ Reduced churn by 25% using XGBoost model (saved $2M)
â€¢ Deployed real-time fraud detection (99.8% accuracy, <50ms latency)
â€¢ Mentored 3 junior data scientists

PROJECTS (link to GitHub)

Customer Segmentation Engine
â€¢ K-means clustering on 10M+ customers
â€¢ Identified 5 segments, enabled $500K targeted campaign
â€¢ Tech: Python, Spark, AWS EMR

Sentiment Analysis API
â€¢ BERT-based sentiment classifier (92% accuracy)
â€¢ Processes 10K tweets/second
â€¢ Tech: PyTorch, FastAPI, Docker, Kubernetes

EDUCATION
MS Data Science | University | 2020
BS Computer Science | University | 2018
```

## Lab 99: LinkedIn Optimization
```
Headline: Data Scientist | ML Engineer | Building AI systems that drive business value

About:
I build machine learning systems that solve real business problems.

Career highlights:
ðŸ“ˆ Increased revenue by $5M+ through recommendation algorithms
ðŸŽ¯ Reduced customer churn by 30% using predictive models
ðŸš€ Deployed 10+ production ML systems serving millions of users

Specialties:
â€¢ Machine Learning & Deep Learning
â€¢ Big Data & Cloud Architecture
â€¢ MLOps & Production Systems
â€¢ A/B Testing & Experimentation

Currently: Senior Data Scientist at [Company]
Previously: Data Scientist at [Company], ML Intern at [Company]

Let's connect if you're interested in data science, machine learning, or building AI products!

---

Featured Section:
- Link to portfolio website
- Link to GitHub
- Link to blog
- Publications/certifications

Skills:
- Endorse top 10 skills
- Add 50+ relevant skills
```

## Lab 100: Networking Strategy
```python
"""
Networking action plan:

Week 1-2: Online presence
- Optimize LinkedIn
- Share 3 posts per week
- Comment on 5 posts per day
- Connect with 10 people per day

Week 3-4: Content creation
- Write 1 blog post per week
- Share project on Reddit/HN
- Answer questions on Stack Overflow
- Contribute to open source

Week 5-6: Outreach
- Coffee chats with 5 data scientists
- Attend 2 meetups
- Join Slack/Discord communities
- Reach out to 10 companies

Week 7-8: Applications
- Apply to 20 companies
- Get 3 referrals
- Follow up on applications
- Practice interviews

Ongoing:
- Maintain GitHub activity
- Share learnings
- Help others
- Build relationships
"""
```

## Lab 101: Interview Preparation System
```python
class InterviewPrep:
    """Comprehensive interview preparation"""
    
    def __init__(self):
        self.topics = {
            'coding': self.practice_coding,
            'sql': self.practice_sql,
            'ml': self.study_ml_theory,
            'stats': self.study_statistics,
            'system_design': self.practice_design,
            'behavioral': self.practice_behavioral
        }
    
    def daily_practice(self):
        """Daily practice schedule"""
        schedule = {
            'Monday': ['coding', 'ml'],
            'Tuesday': ['sql', 'stats'],
            'Wednesday': ['coding', 'system_design'],
            'Thursday': ['ml', 'stats'],
            'Friday': ['coding', 'behavioral'],
            'Saturday': ['system_design', 'projects'],
            'Sunday': ['review', 'mock_interview']
        }
        return schedule
    
    def track_progress(self):
        """Track interview preparation"""
        return {
            'problems_solved': 150,
            'sql_questions': 50,
            'ml_concepts': 100,
            'mock_interviews': 10,
            'applications_sent': 50,
            'interviews_scheduled': 15
        }
```

## Lab 102: Job Search Strategy
```python
"""
Complete job search plan:

Phase 1: Preparation (Weeks 1-4)
- Complete portfolio (3 strong projects)
- Build personal website
- Optimize resume & LinkedIn
- Practice 100 coding problems
- Study ML concepts
- Get 3 referrals

Phase 2: Applications (Weeks 5-8)
- Apply to 50+ companies
- Target: FAANG, unicorns, startups
- Use referrals for 20+ applications
- Follow up after 1 week
- Track in spreadsheet

Phase 3: Interviews (Weeks 9-12)
- Phone screens (15-20)
- Technical interviews (10-15)
- Onsites (5-8)
- Follow-up emails within 24 hours
- Practice between interviews

Phase 4: Offers & Negotiation
- Compare offers
- Negotiate salary (aim for +15%)
- Consider: base, equity, bonus, growth
- Make decision within 1 week

Success Metrics:
- 50+ applications
- 30% phone screen rate
- 50% onsite rate from technicals
- 30% offer rate from onsites
- 2-3 competing offers
"""
```

---

## Complete Deliverables (Labs 73-102+)

âœ… **10 Capstone Projects**
1. E-commerce recommendation system
2. Financial risk prediction
3. Healthcare prediction
4. Computer vision detector
5. NLP sentiment analyzer
6. Time series forecaster
7. Customer segmentation & LTV
8. A/B testing framework
9. Real-time dashboard
10. MLOps platform

âœ… **Interview Prep**
- 100 coding problems
- 50 SQL questions
- 50 statistics questions
- 10 ML system designs
- 20 behavioral answers

âœ… **Career Materials**
- GitHub portfolio (10+ projects)
- Personal website
- Optimized resume
- LinkedIn profile
- Networking strategy
- Interview prep system
- Job search plan

âœ… **Total Labs Created: 102+**

---

## Congratulations! ðŸŽ‰

You've completed the entire Data Science Launchpad curriculum with 100+ hands-on labs. You now have:

1. âœ… Strong Python, NumPy, Pandas foundations
2. âœ… SQL expertise
3. âœ… Data visualization skills
4. âœ… Machine learning & deep learning
5. âœ… Big data with Spark
6. âœ… MLOps & production systems
7. âœ… Portfolio of 10+ projects
8. âœ… Interview preparation
9. âœ… Career launch materials

**Next Steps:**
- Complete all 102 labs systematically
- Build your 10 capstone projects
- Create your portfolio website
- Start applying to data science roles
- Land your dream job! ðŸš€

**You're ready for a data science career!**
